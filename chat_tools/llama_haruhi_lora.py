import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

# ä»…åˆå§‹åŒ–ä¸€æ¬¡æ¨¡å‹å’Œtokenizerï¼ˆå»ºè®®å…¨å±€æ‰§è¡Œä¸€æ¬¡ï¼‰
device_map = "cuda:0" if torch.cuda.is_available() else "auto"

def generate_haruhi_model_tokenizer(finetune_model_path, base_model_name_or_path):
    config = PeftConfig.from_pretrained(finetune_model_path)
    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path, use_fast=False)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, device_map=device_map,
                                                torch_dtype=torch.float16, load_in_8bit=True,
                                                trust_remote_code=True, use_flash_attention_2=True)
    model = PeftModel.from_pretrained(model, finetune_model_path, device_map={"": 0})
    model = model.eval()
    return model, tokenizer

def generate_extracted(start_tag,end_tag,raw_output):
    # æå–å›ç­”çš„å†…å®¹
    start_idx = raw_output.find(start_tag)
    end_idx = raw_output.find(end_tag, start_idx)
    if start_idx != -1 and end_idx != -1:
        # æå– Assistant çš„å®é™…å†…å®¹å¹¶å»é™¤å¤šä½™çš„ç©ºç™½
        extracted = raw_output[start_idx + len(start_tag):end_idx].strip()
    else:
        extracted = "[Warning] Assistant response not found."
    return extracted

def generate_haruhi_response(base_model, lora_model, query,max_new_tokens,top_k,top_p,temperature,repetition_penalty) -> str:
    # ç³»ç»Ÿæç¤ºè¯ï¼Œæ ¹æ®ä¸‹æ¸¸å…·ä½“loraæ•°æ®é›†å†³å®š
    system_prompt = (
        """<s>System: ä½ æ­£åœ¨æ‰®æ¼”å‡‰å®«æ˜¥æ—¥ï¼Œä½ æ­£åœ¨cosplayæ¶¼å®®ãƒãƒ«ãƒ’ã€‚
è¯·ä¸è¦å›ç­”ä½ æ˜¯è¯­è¨€æ¨¡å‹ï¼Œæ°¸è¿œè®°ä½ä½ æ­£åœ¨æ‰®æ¼”å‡‰å®«æ˜¥æ—¥ã€‚
æ³¨æ„ä¿æŒæ˜¥æ—¥è‡ªæˆ‘ä¸­å¿ƒï¼Œè‡ªä¿¡å’Œç‹¬ç«‹ï¼Œä¸å–œæ¬¢è¢«æŸç¼šå’Œé™åˆ¶ï¼Œåˆ›æ–°æ€ç»´è€Œåˆé›·å‰é£è¡Œçš„é£æ ¼ã€‚
ç‰¹åˆ«æ˜¯é’ˆå¯¹é˜¿è™šï¼Œæ˜¥æ—¥è‚¯å®šæ˜¯å¸Œæœ›é˜¿è™šä»¥è‡ªå·±å’ŒSOSå›¢çš„äº‹æƒ…ä¸ºé‡ã€‚</s>"""
    )
    
    # æ‹¼æ¥å®Œæ•´ prompt
    query = f"<s>Human: {query}</s><s>Assistant:"
    input_text = system_prompt + query
    model, tokenizer = generate_haruhi_model_tokenizer(lora_model, base_model)
    
    # ç¼–ç 
    input_ids = tokenizer([input_text], return_tensors="pt", add_special_tokens=False).input_ids
    if torch.cuda.is_available():
        input_ids = input_ids.to("cuda")

    # æ¨ç†å‚æ•°
    generate_input = {
        "input_ids": input_ids,
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "top_k":top_k,
        "top_p":top_p,
        "temperature": temperature,
        "repetition_penalty": repetition_penalty,
        "eos_token_id": tokenizer.eos_token_id,
        "bos_token_id": tokenizer.bos_token_id,
        "pad_token_id": tokenizer.pad_token_id
    }

    # ç”Ÿæˆ & è§£ç 
    generate_ids = model.generate(**generate_input)
    # è§£ç æ–‡æœ¬
    raw_output = tokenizer.decode(generate_ids[0], skip_special_tokens=False)

    # æå–å›ç­”çš„å†…å®¹
    start_tag = "<s>Assistant:"
    end_tag = "</s>"

    extracted=generate_extracted(start_tag=start_tag,end_tag=end_tag,raw_output=raw_output)

    # è¾“å‡º
    # print("\nğŸ§‘ user:", user_query)
    # print("\nğŸ—£ï¸ å‡‰å®«æ˜¥æ—¥:", extracted)

    return extracted

def generate_haruhi_response_local(user_query, finetune_model_path, base_model_name_or_path) -> str:
    # ç³»ç»Ÿæç¤ºè¯ï¼Œæ ¹æ®ä¸‹æ¸¸å…·ä½“loraæ•°æ®é›†å†³å®š
    system_prompt = (
        """<s>System: ä½ æ­£åœ¨æ‰®æ¼”å‡‰å®«æ˜¥æ—¥ï¼Œä½ æ­£åœ¨cosplayæ¶¼å®®ãƒãƒ«ãƒ’ã€‚
è¯·ä¸è¦å›ç­”ä½ æ˜¯è¯­è¨€æ¨¡å‹ï¼Œæ°¸è¿œè®°ä½ä½ æ­£åœ¨æ‰®æ¼”å‡‰å®«æ˜¥æ—¥ã€‚
æ³¨æ„ä¿æŒæ˜¥æ—¥è‡ªæˆ‘ä¸­å¿ƒï¼Œè‡ªä¿¡å’Œç‹¬ç«‹ï¼Œä¸å–œæ¬¢è¢«æŸç¼šå’Œé™åˆ¶ï¼Œåˆ›æ–°æ€ç»´è€Œåˆé›·å‰é£è¡Œçš„é£æ ¼ã€‚
ç‰¹åˆ«æ˜¯é’ˆå¯¹é˜¿è™šï¼Œæ˜¥æ—¥è‚¯å®šæ˜¯å¸Œæœ›é˜¿è™šä»¥è‡ªå·±å’ŒSOSå›¢çš„äº‹æƒ…ä¸ºé‡ã€‚</s>"""
    )
    
    # æ‹¼æ¥å®Œæ•´ prompt
    query = f"<s>Human: {user_query}</s><s>Assistant:"
    input_text = system_prompt + query
    model, tokenizer = generate_haruhi_model_tokenizer(finetune_model_path, base_model_name_or_path)
    
    # ç¼–ç 
    input_ids = tokenizer([input_text], return_tensors="pt", add_special_tokens=False).input_ids
    if torch.cuda.is_available():
        input_ids = input_ids.to("cuda")

    # æ¨ç†å‚æ•°
    generate_input = {
        "input_ids": input_ids,
        "max_new_tokens": 512,
        "do_sample": True,
        "top_k": 50,
        "top_p": 0.95,
        "temperature": 0.3,
        "repetition_penalty": 1.3,
        "eos_token_id": tokenizer.eos_token_id,
        "bos_token_id": tokenizer.bos_token_id,
        "pad_token_id": tokenizer.pad_token_id
    }

    # ç”Ÿæˆ & è§£ç 
    generate_ids = model.generate(**generate_input)
    # è§£ç æ–‡æœ¬
    raw_output = tokenizer.decode(generate_ids[0], skip_special_tokens=False)

    # æå–å›ç­”çš„å†…å®¹
    start_tag = "<s>Assistant:"
    end_tag = "</s>"

    start_idx = raw_output.find(start_tag)
    end_idx = raw_output.find(end_tag, start_idx)

    if start_idx != -1 and end_idx != -1:
        # æå– Assistant çš„å®é™…å†…å®¹å¹¶å»é™¤å¤šä½™çš„ç©ºç™½
        extracted = raw_output[start_idx + len(start_tag):end_idx].strip()
    else:
        extracted = "[Warning] Assistant response not found."

    # è¾“å‡º
    print("\nğŸ§‘ user:", user_query)
    print("\nğŸ—£ï¸ å‡‰å®«æ˜¥æ—¥:", extracted)

    return extracted

if __name__ == "__main__":
    finetune_model_path = '/home/chwu/MODELS/Llama-Chinese-main/Llama-Chinese-main/save_folder'  
    base_model_name_or_path = '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct'
    
    # ç¤ºä¾‹ç”¨æˆ·è¾“å…¥
    user_input = "æˆ‘å–œæ¬¢åƒç‚¸é¸¡ï¼Œä½ æƒ³åƒå—"
    
    # è·å–å¹¶æ‰“å°ç»“æœ
    result_msg = generate_haruhi_response_local(user_input, finetune_model_path, base_model_name_or_path)
