[2025-04-01 14:20:02,696] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:20:02,970] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:20:27,720] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:20:27,908] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:20:27,908] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None 1,0 pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:20:29,268] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:20:29,450] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:20:29,450] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:20:29,450] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:20:29,450] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:20:29,450] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:20:29,450] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:20:29,450] [INFO] [launch.py:253:main] process 3480585 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', '1,0', '--local_rank=0', 'pretrain_clm.py', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:20:30,452] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3480585
[2025-04-01 14:20:30,452] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', '1,0', '--local_rank=0', 'pretrain_clm.py', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 2
[2025-04-01 14:21:02,674] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:21:02,864] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:21:32,098] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:21:32,306] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:22:05,028] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:22:05,214] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:22:14,180] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:22:14,367] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:22:26,352] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:22:26,538] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:22:40,580] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:22:40,791] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:23:10,439] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:23:10,650] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:23:17,324] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:23:17,513] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:23:17,513] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:23:18,882] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:23:19,060] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:23:19,060] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:23:19,060] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:23:19,060] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:23:19,060] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:23:19,060] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:23:19,060] [INFO] [launch.py:253:main] process 3482862 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:23:20,062] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3482862
[2025-04-01 14:23:20,062] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:24:27,719] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:24:27,911] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:24:27,911] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:24:29,300] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:24:29,479] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:24:29,479] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:24:29,479] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:24:29,479] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:24:29,479] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:24:29,479] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:24:29,480] [INFO] [launch.py:253:main] process 3483798 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:24:30,481] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3483798
[2025-04-01 14:24:30,481] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:27:15,995] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:27:16,179] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:27:16,180] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:27:17,527] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:27:17,728] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:27:17,728] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:27:17,728] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:27:17,728] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:27:17,728] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:27:17,728] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:27:17,728] [INFO] [launch.py:253:main] process 3486075 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:27:18,730] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3486075
[2025-04-01 14:27:18,730] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:28:55,143] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:28:55,351] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:28:55,351] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:28:56,699] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:28:56,879] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:28:56,879] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:28:56,879] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:28:56,879] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:28:56,879] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:28:56,879] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:28:56,880] [INFO] [launch.py:253:main] process 3487283 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:28:57,880] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3487283
[2025-04-01 14:28:57,880] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:41:58,011] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:41:58,196] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:41:58,196] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:41:59,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:41:59,738] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:41:59,738] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:41:59,738] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:41:59,738] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:41:59,738] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:41:59,738] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:41:59,739] [INFO] [launch.py:253:main] process 3496211 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:42:00,740] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3496211
[2025-04-01 14:42:00,740] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:43:35,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:43:36,031] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:43:36,031] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:43:37,398] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:43:37,603] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:43:37,603] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:43:37,603] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:43:37,603] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:43:37,603] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:43:37,603] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:43:37,604] [INFO] [launch.py:253:main] process 3497616 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:43:38,604] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3497616
[2025-04-01 14:43:38,604] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 14:53:43,568] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:53:43,750] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 14:53:43,750] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 14:53:45,099] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 14:53:45,276] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 14:53:45,276] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 14:53:45,276] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 14:53:45,276] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 14:53:45,276] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 14:53:45,276] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 14:53:45,277] [INFO] [launch.py:253:main] process 3504640 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 14:53:46,278] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3504640
[2025-04-01 14:53:46,279] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 15:08:13,119] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 15:08:13,301] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 15:08:13,301] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 15:08:14,663] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 15:08:14,842] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 15:08:14,842] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 15:08:14,842] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 15:08:14,842] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 15:08:14,842] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 15:08:14,842] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 15:08:14,842] [INFO] [launch.py:253:main] process 3514847 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 15:08:15,843] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3514847
[2025-04-01 15:08:15,844] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 15:11:40,043] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 15:11:40,228] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 15:11:40,228] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 15:11:41,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 15:11:41,743] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 15:11:41,743] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 15:11:41,743] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 15:11:41,743] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 15:11:41,743] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 15:11:41,743] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 15:11:41,744] [INFO] [launch.py:253:main] process 3517359 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 15:11:42,745] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3517359
[2025-04-01 15:11:42,745] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
[2025-04-01 16:01:12,171] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 16:01:12,425] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-04-01 16:01:12,425] [INFO] [runner.py:568:main] cmd = /home/anaconda3/envs/llama-chinese/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_clm.py --config_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json --tokenizer_name /home/chwu/MODELS/Llama3-Chinese-8B-Instruct --train_files ../../data/wiki_zh/train_hari_v2.csv --validation_files ../../data/wiki_zh/dev_hari_v2.csv --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --do_train --output_dir ../../output_model --evaluation_strategy steps --use_fast_tokenizer false --max_eval_samples 500 --learning_rate 1e-4 --gradient_accumulation_steps 2 --num_train_epochs 3 --warmup_steps 5000 --logging_dir ../../output_model/logs --logging_strategy steps --logging_steps 5 --save_strategy steps --preprocessing_num_workers 10 --save_steps 500 --eval_steps 5000000 --save_total_limit 2000 --seed 42 --disable_tqdm false --ddp_find_unused_parameters false --block_size 1024 --overwrite_output_dir --report_to tensorboard --run_name ../../output_model --bf16 --bf16_full_eval --gradient_checkpointing --deepspeed ./ds_config_zero3.json --ignore_data_skip true --ddp_timeout 18000000
[2025-04-01 16:01:13,867] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-01 16:01:14,074] [INFO] [launch.py:138:main] 0 NCCL_P2P_DISABLE=1
[2025-04-01 16:01:14,074] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2025-04-01 16:01:14,074] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2025-04-01 16:01:14,074] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2025-04-01 16:01:14,074] [INFO] [launch.py:163:main] dist_world_size=1
[2025-04-01 16:01:14,074] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2025-04-01 16:01:14,074] [INFO] [launch.py:253:main] process 3528423 spawned with command: ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000']
[2025-04-01 16:01:16,076] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3528423
[2025-04-01 16:01:16,076] [ERROR] [launch.py:322:sigkill_handler] ['/home/anaconda3/envs/llama-chinese/bin/python', '-u', 'pretrain_clm.py', '--local_rank=0', '--config_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct/config.json', '--tokenizer_name', '/home/chwu/MODELS/Llama3-Chinese-8B-Instruct', '--train_files', '../../data/wiki_zh/train_hari_v2.csv', '--validation_files', '../../data/wiki_zh/dev_hari_v2.csv', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '32', '--do_train', '--output_dir', '../../output_model', '--evaluation_strategy', 'steps', '--use_fast_tokenizer', 'false', '--max_eval_samples', '500', '--learning_rate', '1e-4', '--gradient_accumulation_steps', '2', '--num_train_epochs', '3', '--warmup_steps', '5000', '--logging_dir', '../../output_model/logs', '--logging_strategy', 'steps', '--logging_steps', '5', '--save_strategy', 'steps', '--preprocessing_num_workers', '10', '--save_steps', '500', '--eval_steps', '5000000', '--save_total_limit', '2000', '--seed', '42', '--disable_tqdm', 'false', '--ddp_find_unused_parameters', 'false', '--block_size', '1024', '--overwrite_output_dir', '--report_to', 'tensorboard', '--run_name', '../../output_model', '--bf16', '--bf16_full_eval', '--gradient_checkpointing', '--deepspeed', './ds_config_zero3.json', '--ignore_data_skip', 'true', '--ddp_timeout', '18000000'] exits with return code = 1
